{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate and Multivariate Statistics with NHANES Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Data wrangling is the process of transforming and mapping data from a raw data form into another format with the intent of making it more appropriate and valuable for data analytics.\n",
    "\n",
    "1. Data Discovery\n",
    "    \n",
    "    This all-encompassing term describes how to understand your data. This is the first step to familiarize yourself with your data. You will use the Data Dictionary and different PANDAS commands for this step:\n",
    "    \n",
    "\n",
    "2. Structuring \n",
    "    \n",
    "    The next step is to organize the data. Raw data is typically unorganized and much of it may not be useful for the end product. This step is important for easier computation and analysis in the later steps.\n",
    "    \n",
    "\n",
    "3. Cleaning \n",
    "    \n",
    "    There are many different forms of cleaning data, for example one form of cleaning data is catching dates formatted in a different way and another form is removing outliers that will skew results and also formatting null values. This step is important in assuring the overall quality of the data.\n",
    "    \n",
    "\n",
    "4. Enriching \n",
    "    \n",
    "    At this step determine whether or not additional data would benefit the data set that could be easily added.\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will use PANDAS to complete these data wrangling steps.\n",
    "\n",
    "### PANDAS First Steps Import\n",
    "\n",
    " PANDAS is a module of code that can be used in python.  To use this module or any other module you have to import the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#set pandas to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#set pandas to display 250 rows\n",
    "pd.set_option('display.max_rows', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"assets/modified_NHANES.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Discovery\n",
    "\n",
    "### Viewing your data\n",
    "\n",
    "The first thing to do when opening a new dataset is print out a few rows to keep as a visual reference. We accomplish this with `.head()`\n",
    "\n",
    "df.head() outputs the first five rows of your DataFrame by default, but we could also pass a number as well: df.head(20) would output the top twenty rows.\n",
    "\n",
    "To see the last five rows use .tail(). tail() also accepts a number to display the desired last rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need to determine how many rows and columns make up the dataframe.  We do this by using the `.shape` method. Note that `.shape` has no parentheses and is returns (rows, columns). \n",
    "\n",
    "You'll be going to `.shape` a lot when cleaning and transforming data. For example, you might filter some rows based on some criteria and then want to know quickly how many rows were subsetted, added, or removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "We need to determine how each column is coded.  \n",
    "\n",
    "- Do the columns contain continuous data or catagorical data?  \n",
    "- Which of these columns are important to our project question or hypothesis?\n",
    "- How do we need to process our data to be usable (if it is a catagorical column  of 1-6, 9. What do those numbers mean?\n",
    "\n",
    "You can open the NHANES Abbreviated Data Dictionary Word file in the assets directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring and Cleaning Data\n",
    "\n",
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your dataset should not have duplicate rows, but it is always important to verify you aren't aggregating duplicate rows.  \n",
    "\n",
    "`df.duplicated()` returns whether or not the row is duplicated\n",
    "\n",
    "`df.duplicated().sum()` returns the number or rows duplicated\n",
    "\n",
    "`df[df.duplicated()]` return the duplicated rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset did not have duplicated rows.  If it did you would run \n",
    "\n",
    "`df = df.drop_duplicates()` to remove the duplicated rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.info()` provides the essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using. \n",
    "\n",
    "Notice in our movies dataset we have some obvious missing values in the `PAT_CTY_CODE` and `SEX_CODE` columns as well as others. We'll look at how to handle those in a bit.\n",
    "\n",
    "Seeing the datatype quickly is actually quite useful.  Calling `.info()` will quickly point out that your column you thought was all integers are actually string objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with missing values\n",
    "\n",
    "When exploring data, youâ€™ll most likely encounter missing or null values, which are essentially placeholders for non-existent values. Most commonly you'll see Python's `None` or NumPy's `np.nan`, each of which are handled differently in some situations.\n",
    "\n",
    "There are two options in dealing with nulls: \n",
    "\n",
    "1. Get rid of rows or columns with nulls\n",
    "2. Replace nulls with non-null values, a technique known as **imputation**\n",
    "\n",
    "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our DataFrame are null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the variable DMDEDUC2, we can look at the counts of each code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DMDEDUC2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.DMDEDUC2.value_counts().sum())\n",
    "print(1621 + 1366 + 1186 + 655 + 643 + 3) # Manually sum the frequencies\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine the number of entries that are missing or Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.isnull(df.DMDEDUC2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding Data\n",
    "\n",
    "We can recode the data preventing the loss of data from other columns.  For example, we can recode DMDEDUC2.  Look at the data dictionary that describes how this is coded.  \n",
    "\n",
    "First we will recode the data that is not missing so that it is easier to read.  We will recode the data into a new column `DMDEDUC2x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"DMDEDUC2x\"] = df.DMDEDUC2.replace({1: \"<9\", 2: \"9-11\", 3: \"HS/GED\", 4: \"Some college/AA\", 5: \"College\", \n",
    "                                       7: \"Refused\", 9: \"Don't know\"})\n",
    "df.DMDEDUC2x.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do replace missing or null data with the term 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DMDEDUC2x\"] = df.DMDEDUC2x.fillna(\"Missing\")\n",
    "\n",
    "print(df.DMDEDUC2x.value_counts().sum())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recode RIAGENDR to RIAGENDRx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"RIAGENDRx\"] = df.RIAGENDR.replace({1: \"Male\", 2: \"Female\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DMDHHSIZ.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows that have missing data that are important for our project - Height, Weight , BMI\n",
    "\n",
    "We can describe the data that is not missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BMXWT.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df.BMXWT.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df.BPXSY1.dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = sns.boxplot(data=df.loc[:, [\"BPXSY1\", \"BPXSY2\", \"BPXDI1\", \"BPXDI2\"]])\n",
    "_ = bp.set_ylabel(\"Blood pressure in mm/Hg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"agegrp\"] = pd.cut(df.RIDAGEYR, [18, 30, 40, 50, 60, 70, 80]) # Create age strata based on these cut points\n",
    "plt.figure(figsize=(12, 5))  # Make the figure wider than default (12cm wide by 5cm tall)\n",
    "sns.boxplot(x=\"agegrp\", y=\"BPXSY1\", data=df)  # Make boxplot of BPXSY1 stratified by age group\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"agegrp\"] = pd.cut(df.RIDAGEYR, [18, 30, 40, 50, 60, 70, 80])\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(x=\"agegrp\", y=\"BPXSY1\", hue=\"RIAGENDRx\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"agegrp\"] = pd.cut(df.RIDAGEYR, [18, 30, 40, 50, 60, 70, 80])\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(x=\"RIAGENDRx\", y=\"BPXSY1\", hue=\"agegrp\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"agegrp\")[\"DMDEDUC2x\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = df.loc[~df.DMDEDUC2x.isin([\"Don't know\", \"Missing\"]), :]  # Eliminate rare/missing values\n",
    "dx = dx.groupby([\"agegrp\", \"RIAGENDRx\"])[\"DMDEDUC2x\"]\n",
    "dx = dx.value_counts()\n",
    "dx = dx.unstack() # Restructure the results from 'long' to 'wide'\n",
    "\n",
    "dx = dx.apply(lambda x: (x/x.sum())*100, axis=1) # Normalize within each stratum to get proportions\n",
    "print(dx.to_string(float_format=\"%.3f\"))  # Limit display to 3 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"BMXLEG\", y=\"BMXARML\", data=df, fit_reg=False, scatter_kws={\"alpha\": 0.2})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(df, col=\"RIAGENDRx\").map(plt.scatter, \"BMXLEG\", \"BMXARML\", alpha=0.4).add_legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
